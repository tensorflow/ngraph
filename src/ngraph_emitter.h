/*******************************************************************************
 * Copyright 2017-2018 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

#ifndef NGRAPH_EMITTER_H_
#define NGRAPH_EMITTER_H_

#include <string>
#include <vector>
#include "ngraph/ngraph.hpp"
#include "ngraph_log.h"
#include "ngraph_op_handler.h"
#include "ngraph_utils.h"
#include "ngraph_xla_compat.h"
#include "tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h"
#include "tensorflow/compiler/xla/service/hlo_computation.h"
#include "tensorflow/compiler/xla/service/hlo_instruction.h"
#include "tensorflow/compiler/xla/service/hlo_module.h"
#include "tensorflow/compiler/xla/status.h"

namespace xla {
namespace ngraph_plugin {

class NGraphBuilder;
//---------------------------------------------------------------------------
// Use a private derivation model so that no one can use the NGraphEmitter as
// is. Only way to use the emitter is to use the NGraphBuilder and it's
// Visitor() method.
// The underlying reason is that before calling any methods on an object of this
// class, the aller must call Initialize(). There is no way to ensure that
// (except for a check in every function which is error prone). This is more
// elegant, simple, and safe.
//---------------------------------------------------------------------------
class NGraphEmitter : private NGraphOpHandler {
 public:
  class FusedOpEmitter;

  // A map which associates each HLO fusion instruction generated by the
  // NGraphFusion pass with the fused-op emitter that will be used to
  // generate its code.
  using FusedOpMap = std::unordered_map<const HloInstruction*,
                                        std::unique_ptr<const FusedOpEmitter>>;

 private:
  NGraphEmitter(const std::vector<HloInstruction*>& hlo_param_list,
                FusedOpMap* fusion_map)
      : m_hlo_parameter_list(hlo_param_list), m_fusion_map(fusion_map) {}
  ~NGraphEmitter() {}

  Status Initialize();

  bool Visited() { return m_graph_visited; }
  const std::unordered_map<const HloInstruction*,
                           std::shared_ptr<ngraph::Node>>&
  OpMap() const {
    return m_op_map;
  }

  const std::map<int64, const HloInstruction*>& ParameterNumberMap() const {
    return m_parameter_number_map;
  }

  StatusOr<std::shared_ptr<compat::XLAFunction>> NGraphFunction(
      const HloInstruction* root_instruction);

  // TODO: CLEANUP Remove Debug
  void DebugPrintInstructionsList() {
    NGRAPH_VLOG(3) << "HLO_INSTRUCTIONS";
    for (const auto& next : m_instruction_list) {
      NGRAPH_VLOG(3) << "NGRAPH Node: " << next.NGraphInstruction
                     << " HLO INSTRUCTION: " << next.HloInstruction;
    }
  }

  // If we are given an instruction that we do not support then log that and
  // return Unimplemented
  Status DefaultAction(HloInstruction* hlo) override {
    NGRAPH_VLOG(1) << "UNEXPECTED!!!! NGraphEmitter::DefaultAction() called."
                   << " Instruction: " << hlo->ToString();
    return Unimplemented("Unhandled instruction '%s'. ",
                         hlo->ToString().c_str());
  }

  Status FinishVisit(HloInstruction* hlo_root) override {
    m_graph_visited = true;
    return NGraphOpHandler::FinishVisit(hlo_root);
  }

  Status ProcessElementwiseUnary(HloInstruction* hlo,
                                 HloOpcode opcode) override;

  Status ProcessElementwiseBinary(HloInstruction* hlo,
                                  HloOpcode opcode) override;

  Status ProcessConcatenate(
      HloInstruction* concatenate,
      tensorflow::gtl::ArraySlice<HloInstruction*> /*operands*/) override;

  Status ProcessConvert(HloInstruction* convert) override;

  Status ProcessReverse(HloInstruction* reverse,
                        const HloInstruction* operand) override;

  Status ProcessConvolution(HloInstruction* convolution,
                            const HloInstruction* lhs,
                            const HloInstruction* rhs,
                            const Window& window) override;

  Status ProcessSelect(HloInstruction* select, const HloInstruction* pred,
                       const HloInstruction* on_true,
                       const HloInstruction* on_false) override;

  Status ProcessDot(HloInstruction* dot, const HloInstruction* lhs,
                    const HloInstruction* rhs) override;

  Status ProcessCompare(HloInstruction* compare, HloOpcode opcode,
                        const HloInstruction* lhs,
                        const HloInstruction* rhs) override;

  Status ProcessConstant(HloInstruction* constant,
                         const Literal& literal) override;

  Status ProcessGetTupleElement(HloInstruction* get_tuple_element,
                                const HloInstruction* operand) override;

  Status ProcessParameter(HloInstruction* parameter) override;

  Status ProcessSlice(HloInstruction* slice,
                      const HloInstruction* /*operand*/) override;

  Status ProcessTuple(
      HloInstruction* tuple,
      tensorflow::gtl::ArraySlice<HloInstruction*> operands) override;

  Status ProcessReduce(HloInstruction* reduce, const HloInstruction* /*arg*/,
                       const HloInstruction* /*init_value*/,
                       const std::vector<int64>& /*dimensions*/,
                       const HloComputation* /*function*/) override;

  Status ProcessReduceWindow(HloInstruction* reduce_window,
                             const HloInstruction* operand,
                             const Window& window,
                             const HloComputation* function) override;

  Status ProcessSelectAndScatter(HloInstruction* select_and_scatter) override;

  Status ProcessBroadcast(HloInstruction* broadcast) override;

  Status ProcessReshape(HloInstruction* reshape) override;

  Status ProcessTranspose(HloInstruction* transpose) override;

  Status ProcessPad(HloInstruction* pad) override;

  Status ProcessBatchNormTraining(HloInstruction* bnt) override;

  Status ProcessBatchNormInference(HloInstruction* bni) override;

  Status ProcessBatchNormGrad(HloInstruction* bng) override;

  Status ProcessFusion(HloInstruction* fusion) override;

 public:
  // Fused-op emitters encapsulate code generation for the fused ops generated
  // in the NGraphFusion pass.
  class FusedOpEmitter {
   public:
    virtual ~FusedOpEmitter() {}

    // Emit nGraph nodes for this fused op, and return the root nGraph node.
    virtual StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const = 0;
  };

  // Fused-op emitter for binop reduction operations.
  class FusedReductionEmitter : public FusedOpEmitter {
   public:
    FusedReductionEmitter(HloOpcode opcode,
                          const std::vector<int64>& dimensions)
        : m_opcode(opcode), m_dimensions(dimensions) {}
    ~FusedReductionEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    HloOpcode m_opcode;
    std::vector<int64> m_dimensions;
  };

  // Fused-op emitter for max-pool.
  class FusedMaxPoolEmitter : public FusedOpEmitter {
   public:
    FusedMaxPoolEmitter(const Window& window, bool is_nchw)
        : m_window(window), m_is_nchw(is_nchw) {}
    ~FusedMaxPoolEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    Window m_window;
    bool m_is_nchw;
  };

  // Fused-op emitter for max-pool backprop.
  // TODO(amprocte): add provisions for matching forward-prop op.
  class FusedMaxPoolBackpropEmitter : public FusedOpEmitter {
   public:
    FusedMaxPoolBackpropEmitter(const Window& window, bool is_nchw)
        : m_window(window), m_is_nchw(is_nchw) {}
    ~FusedMaxPoolBackpropEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    Window m_window;
    bool m_is_nchw;
  };

  // Fused-op emitter for avg-pool.
  class FusedAvgPoolEmitter : public FusedOpEmitter {
   public:
    FusedAvgPoolEmitter(const Window& window, bool is_nchw)
        : m_window(window), m_is_nchw(is_nchw) {}
    ~FusedAvgPoolEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    Window m_window;
    bool m_is_nchw;
  };

  // Fused-op emitter for avg-pool backprop.
  class FusedAvgPoolBackpropEmitter : public FusedOpEmitter {
   public:
    FusedAvgPoolBackpropEmitter(const xla::Shape& forward_arg_shape,
                                const Window& window, bool is_nchw)
        : m_forward_arg_shape(forward_arg_shape),
          m_window(window),
          m_is_nchw(is_nchw) {}
    ~FusedAvgPoolBackpropEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    xla::Shape m_forward_arg_shape;
    Window m_window;
    bool m_is_nchw;
  };

  // Fused-op emitter for backprop to convolution image batch.
  //
  // TODO(amprocte): the parameters should be expressed in terms of XLA notions
  // of windows and shapes, not nGraph types.
  class FusedConvBackpropInputEmitter : public FusedOpEmitter {
   public:
    FusedConvBackpropInputEmitter(
        const ngraph::Shape& forward_input_shape,
        const ngraph::Strides& forward_window_movement_strides,
        const ngraph::CoordinateDiff& forward_padding_below,
        const ngraph::CoordinateDiff& forward_padding_above, bool is_nchw)
        : m_forward_input_shape(forward_input_shape),
          m_forward_window_movement_strides(forward_window_movement_strides),
          m_forward_padding_below(forward_padding_below),
          m_forward_padding_above(forward_padding_above),
          m_is_nchw(is_nchw) {}
    ~FusedConvBackpropInputEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    ngraph::Shape m_forward_input_shape;
    ngraph::Strides m_forward_window_movement_strides;
    ngraph::CoordinateDiff m_forward_padding_below;
    ngraph::CoordinateDiff m_forward_padding_above;
    bool m_is_nchw;
  };

  // Fused-op emitter for backprop to convolution filters.
  //
  // TODO(amprocte): the parameters should be expressed in terms of XLA notions
  // of windows and shapes, not nGraph types.
  class FusedConvBackpropFiltersEmitter : public FusedOpEmitter {
   public:
    FusedConvBackpropFiltersEmitter(
        const ngraph::Shape& forward_filters_shape,
        const ngraph::Strides& forward_window_movement_strides,
        const ngraph::CoordinateDiff& forward_padding_below,
        const ngraph::CoordinateDiff& forward_padding_above, bool is_nchw)
        : m_forward_filters_shape(forward_filters_shape),
          m_forward_window_movement_strides(forward_window_movement_strides),
          m_forward_padding_below(forward_padding_below),
          m_forward_padding_above(forward_padding_above),
          m_is_nchw(is_nchw) {}
    ~FusedConvBackpropFiltersEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;

   private:
    ngraph::Shape m_forward_filters_shape;
    ngraph::Strides m_forward_window_movement_strides;
    ngraph::CoordinateDiff m_forward_padding_below;
    ngraph::CoordinateDiff m_forward_padding_above;
    bool m_is_nchw;
  };

  // Fused-op emitter for ReLU forward prop.
  class FusedReluEmitter : public FusedOpEmitter {
   public:
    FusedReluEmitter() {}
    ~FusedReluEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;
  };

  // Fused-op emitter for ReLU backward prop.
  class FusedReluBackpropEmitter : public FusedOpEmitter {
   public:
    FusedReluBackpropEmitter() {}
    ~FusedReluBackpropEmitter() override {}

    StatusOr<std::shared_ptr<ngraph::Node>> Emit(
        const HloInstruction* instruction,
        const std::unordered_map<const HloInstruction*,
                                 std::shared_ptr<ngraph::Node>>& op_map)
        const override;
  };

 private:
  TF_DISALLOW_COPY_AND_ASSIGN(NGraphEmitter);

  static StatusOr<ngraph::AxisVector> GetConvolutionImagebatchShuffleVector(
      HloInstruction* convolution);

  static StatusOr<ngraph::AxisVector> GetConvolutionFiltersShuffleVector(
      HloInstruction* convolution);

  static StatusOr<ngraph::AxisVector> GetConvolutionOutputShuffleVector(
      HloInstruction* convolution);

  static StatusOr<std::shared_ptr<ngraph::Node>> MakeNGraphConstant(
      const xla::Shape& xla_shape, const Literal& literal);

  StatusOr<std::shared_ptr<ngraph::op::Broadcast>> MakeNGBroadcastOp(
      const HloInstruction* operand,
      const std::shared_ptr<ngraph::Node>& ng_operand,
      const std::vector<int64>& broadcast_dimensions,
      const Shape& broadcast_shape) const;

  // Stores list of processed instruction (str) pairs for debugging
  typedef struct {
    string HloInstruction;
    string NGraphInstruction;
  } DbgInstructionItem;
  std::vector<DbgInstructionItem> m_instruction_list;

  // Main map, mapping Hlo instruction to ngraph node
  std::unordered_map<const HloInstruction*, std::shared_ptr<ngraph::Node>>
      m_op_map;

  // Each parameter instruction has a parameter_number - the position of the
  // parameter of the function call
  std::map<int64, const HloInstruction*> m_parameter_number_map;

  // Used to store and retrive tuple instruction
  std::unordered_map<const HloInstruction*,
                     std::vector<std::shared_ptr<ngraph::Node>>>
      m_tuple_op_map_vector;
  // Save the paramater list for processing during Initailize()
  const std::vector<HloInstruction*>& m_hlo_parameter_list;
  // Flag to indicate that the graph has been visited
  bool m_graph_visited{false};
  // Fusion map generated by the NGraphFusion pass
  FusedOpMap* m_fusion_map;
  friend class NGraphBuilder;
};

//---------------------------------------------------------------------------
// This class provides the DfsHloVisitor inplementation so that a
// TF graph can be processed and corresponding nGraph can be built
// from that. Also provides a helper function to create the NGraph
// executable function. Note that the TF graph must have been
// traversed (visted) before creating the nGraph function.
//---------------------------------------------------------------------------
class NGraphBuilder {
 public:
  NGraphBuilder(const std::vector<HloInstruction*>& hlo_param_list,
                NGraphEmitter::FusedOpMap* fusion_map)
      : m_emitter(hlo_param_list, fusion_map) {}

  StatusOr<DfsHloVisitor*> Visitor() {
    TF_CHECK_OK(m_emitter.Initialize());
    return m_emitter.GetVisitor();
  }

  // TODO: CLEANUP Remove Debug
  void DebugPrintInstructionsList() { m_emitter.DebugPrintInstructionsList(); }

  StatusOr<std::shared_ptr<compat::XLAFunction>> NGraphFunction(
      const HloInstruction* root_instruction) {
    if (!m_emitter.Visited()) {
      return FailedPrecondition(
          "The HLO graph hasn't been processed by NGraphEmitter. Get the "
          "Visitor() and run HloInstruction->Accept() first before creating "
          "NGraphFunction.");
    }
    return m_emitter.NGraphFunction(root_instruction);
  }

 private:
  NGraphEmitter m_emitter;
};

}  // namespace ngraph_plugin
}  // namespace xla

#endif  // NGRAPH_EMITTER_H_
